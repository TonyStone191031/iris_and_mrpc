{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、读取csv，转化成Alpaca.json格式的llm训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/msr_paraphrase_train.tsv',sep='\\t',on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看最后5行数据\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个指令\n",
    "instruction=\"判断两个句子在语义上是否等同。如果等价，则输出 “1”；如果不等价，则输出 “0”。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空列表来存储转换后的数据\n",
    "alpaca_data = []\n",
    "for index, row in df.iterrows():\n",
    "    # 创建一个字典来存储当前行的数据\n",
    "    data_point = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": f\"句子 1: {row['#1 String']}\\n句子 2: {row['#2 String']}\",\n",
    "        \"output\": str(row['Quality'])\n",
    "    }\n",
    "    # 将字典添加到列表中\n",
    "    alpaca_data.append(data_point)\n",
    "\n",
    "# 将列表转换为JSON字符串\n",
    "alpaca_json = json.dumps(alpaca_data,ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存到文件\n",
    "with open('./MRPC_train_data.json', 'w') as f:\n",
    "    f.write(alpaca_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、llmam_factory导入数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `LLama-factory/data/dataset_info.json`\n",
    "* 将/output/MRPC_train_data.json文件复制到llamafactory的data目录下\n",
    "> 修改 LLaMaFactory/data/dataset_info.json\n",
    "```json\n",
    "{\n",
    "  \"MRPC_train_data\":{\n",
    "     \"file_name\": \"MRPC_train_data.json\"\n",
    "  }, \n",
    "  \"identity\": {\n",
    "    \"file_name\": \"identity.json\"\n",
    "  ......\n",
    "```\n",
    "将llm_train_data.json 记录添加到配置信息中\n",
    "\n",
    "<img src=\"../res/1.png\" alt=\"Alt Text\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、启动llama_factory可视化界面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda activate /data/lilk/yuexiang/venv\n",
    "\n",
    "cd /data/lilk/yuexiang/LLaMA-Factory\n",
    "\n",
    "python src/webui.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、启动llama_factory进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 方式1 ** 通过平台提供的 Llamafactory 界面训练\n",
    "\n",
    "* 进入界面后，选择模型，填写模型路径，选择训练集\n",
    "  <img src=\"../res/3.jpg\" alt=\"Alt Text\" width=\"800\" height=\"400\">\n",
    "\n",
    "* 根据您的微调经验，调整相关参数，执行训练\n",
    "\n",
    "   <img src=\"../res/4.jpg\" alt=\"Alt Text\" width=\"800\" height=\"400\">\n",
    "\n",
    "* 注意, 您需要将ui中生成的脚本复制到output/目录下的train.sh\n",
    "* 在网页中点击“预览命令”即可出现所对应的命令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 方式2 ** 直接在命令行中执行\n",
    "* 新建 jupyter 终端 \n",
    "* 进入 `/root/dg/LLaMA-Factory` 目录\n",
    "* 命令行下启动微调，例如：\n",
    "```bash\n",
    "llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /home/public/data/Model/Qwen1.5-1.8B-Chat \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MRPC_train_data \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen1.5-1.8B-Chat/lora/test_train1 \\\n",
    "    --bf16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --optim adamw_torch \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五、transformers推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel,LoraConfig,TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data/lilk/yuexiang/Qwen1.5-1.8B-Chat'\n",
    "lora_path = '/data/lilk/yuexiang/LLaMA-Factory/saves/Qwen1.5-1.8B-Chat/lora/train_2024-11-06-17-19-45'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=16, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.# Dropout 比例\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda\",torch_dtype=torch.bfloat16)\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "print(\"模型所在的device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六、读取测试集推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/lilk/yuexiang/data/msr_paraphrase_test.tsv',sep='\\t',on_bad_lines='skip')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 定义调用 LLM 模型的函数\n",
    "def get_llm_response(row):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\":  f\"句子 1: {row['#1 String']}\\n句子 2: {row['#2 String']}\"}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "response_records = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"获取 LLM 回答并更新预测\"):\n",
    "    id = row['#2 ID']\n",
    "    response = get_llm_response(row)\n",
    "    response_records.append({'ID': id, 'preQuality': response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将更新后的记录写入文件\n",
    "output_file = './test_data_llm_predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = pd.DataFrame(response_records)\n",
    "updated_df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
